{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nD17 과제\\n- 이 체인은 영화 제목을 가져와 영화를 나타내는 세 개의 이모티콘으로 응답해야 합니다. (예: \"탑건\" -> \"🛩️👨\\u200d✈️🔥\". \"대부\" -> \"👨\\u200d👨\\u200d👦🔫🍝\").\\n- 앞서 배운 메모리 클래스 중 하나를 사용하는 메모리로 LCEL 체인을 구현합니다.\\n- 항상 세 개의 이모티콘으로 답장하도록 FewShotPromptTemplate 또는 FewShotChatMessagePromptTemplate을 사용하여 체인에 예시를 제공하세요.\\n- 메모리가 작동하는지 확인하려면 체인에 두 개의 영화에 대해 질문한 다음 다른 셀에서 체인에 먼저 질문한 영화가 무엇인지 알려달라고 요청하세요.\\n\\n작업 순서\\n1. memory 없이 먼저 구현. d16 기준\\n2. 강의 듣고, memory 구현\\n\\n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "D17 과제\n",
    "- 이 체인은 영화 제목을 가져와 영화를 나타내는 세 개의 이모티콘으로 응답해야 합니다. (예: \"탑건\" -> \"🛩️👨‍✈️🔥\". \"대부\" -> \"👨‍👨‍👦🔫🍝\").\n",
    "- 앞서 배운 메모리 클래스 중 하나를 사용하는 메모리로 LCEL 체인을 구현합니다.\n",
    "- 항상 세 개의 이모티콘으로 답장하도록 FewShotPromptTemplate 또는 FewShotChatMessagePromptTemplate을 사용하여 체인에 예시를 제공하세요.\n",
    "- 메모리가 작동하는지 확인하려면 체인에 두 개의 영화에 대해 질문한 다음 다른 셀에서 체인에 먼저 질문한 영화가 무엇인지 알려달라고 요청하세요.\n",
    "\n",
    "작업 순서\n",
    "1. memory 없이 먼저 구현. d16 기준\n",
    "2. 강의 듣고, memory 구현\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Single '}' encountered in format string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 99\u001b[0m\n\u001b[1;32m     78\u001b[0m example_prompt \u001b[38;5;241m=\u001b[39m FewShotChatMessagePromptTemplate(\n\u001b[1;32m     79\u001b[0m     examples\u001b[38;5;241m=\u001b[39mexamples,\n\u001b[1;32m     80\u001b[0m     example_prompt\u001b[38;5;241m=\u001b[39mexample_prompt,\n\u001b[1;32m     81\u001b[0m )\n\u001b[1;32m     83\u001b[0m final_prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[1;32m     84\u001b[0m     [\n\u001b[1;32m     85\u001b[0m         (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m     ]\n\u001b[1;32m     97\u001b[0m )\n\u001b[0;32m---> 99\u001b[0m general_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mChatPromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mMessagesPlaceholder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchat_history\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{question}\u001b[39;49;00m\u001b[38;5;124;43m}\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_memory\u001b[39m(_):\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m memory\u001b[38;5;241m.\u001b[39mload_memory_variables({})[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/GitHub/py_w08_fullstack_gpt_d15/env/lib/python3.11/site-packages/langchain/prompts/chat.py:541\u001b[0m, in \u001b[0;36mChatPromptTemplate.from_messages\u001b[0;34m(cls, messages)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_messages\u001b[39m(\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m    505\u001b[0m     messages: Sequence[MessageLikeRepresentation],\n\u001b[1;32m    506\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatPromptTemplate:\n\u001b[1;32m    507\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m        a chat prompt template\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 541\u001b[0m     _messages \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43m_convert_to_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;66;03m# Automatically infer input variables from messages\u001b[39;00m\n\u001b[1;32m    544\u001b[0m     input_vars: Set[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/py_w08_fullstack_gpt_d15/env/lib/python3.11/site-packages/langchain/prompts/chat.py:541\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_messages\u001b[39m(\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m    505\u001b[0m     messages: Sequence[MessageLikeRepresentation],\n\u001b[1;32m    506\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatPromptTemplate:\n\u001b[1;32m    507\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m        a chat prompt template\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 541\u001b[0m     _messages \u001b[38;5;241m=\u001b[39m [\u001b[43m_convert_to_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages]\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;66;03m# Automatically infer input variables from messages\u001b[39;00m\n\u001b[1;32m    544\u001b[0m     input_vars: Set[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/py_w08_fullstack_gpt_d15/env/lib/python3.11/site-packages/langchain/prompts/chat.py:739\u001b[0m, in \u001b[0;36m_convert_to_message\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m    737\u001b[0m message_type_str, template \u001b[38;5;241m=\u001b[39m message\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message_type_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 739\u001b[0m     _message \u001b[38;5;241m=\u001b[39m \u001b[43m_create_template_from_message_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_type_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    741\u001b[0m     _message \u001b[38;5;241m=\u001b[39m message_type_str(prompt\u001b[38;5;241m=\u001b[39mPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(template))\n",
      "File \u001b[0;32m~/Documents/GitHub/py_w08_fullstack_gpt_d15/env/lib/python3.11/site-packages/langchain/prompts/chat.py:692\u001b[0m, in \u001b[0;36m_create_template_from_message_type\u001b[0;34m(message_type, template)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a message prompt template from a message type and template string.\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \n\u001b[1;32m    684\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;124;03m    a message prompt template of the appropriate type.\u001b[39;00m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_type \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 692\u001b[0m     message: BaseMessagePromptTemplate \u001b[38;5;241m=\u001b[39m \u001b[43mHumanMessagePromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemplate\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m message_type \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mai\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    696\u001b[0m     message \u001b[38;5;241m=\u001b[39m AIMessagePromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(template)\n",
      "File \u001b[0;32m~/Documents/GitHub/py_w08_fullstack_gpt_d15/env/lib/python3.11/site-packages/langchain/prompts/chat.py:152\u001b[0m, in \u001b[0;36mBaseStringMessagePromptTemplate.from_template\u001b[0;34m(cls, template, template_format, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_template\u001b[39m(\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[MessagePromptTemplateT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    141\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MessagePromptTemplateT:\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a class from a string template.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m        A new instance of this class.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[43mPromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(prompt\u001b[38;5;241m=\u001b[39mprompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/GitHub/py_w08_fullstack_gpt_d15/env/lib/python3.11/site-packages/langchain/prompts/prompt.py:232\u001b[0m, in \u001b[0;36mPromptTemplate.from_template\u001b[0;34m(cls, template, template_format, partial_variables, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_template\u001b[39m(\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    203\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptTemplate:\n\u001b[1;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a prompt template from a template.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    *Security warning*: Prefer using `template_format=\"f-string\"` instead of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m        The prompt template loaded from the template.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m \u001b[43mget_template_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     _partial_variables \u001b[38;5;241m=\u001b[39m partial_variables \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _partial_variables:\n",
      "File \u001b[0;32m~/Documents/GitHub/py_w08_fullstack_gpt_d15/env/lib/python3.11/site-packages/langchain/prompts/base.py:143\u001b[0m, in \u001b[0;36mget_template_variables\u001b[0;34m(template, template_format)\u001b[0m\n\u001b[1;32m    141\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m _get_jinja2_variables_from_template(template)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m template_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf-string\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 143\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mFormatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported template format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_format\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/py_w08_fullstack_gpt_d15/env/lib/python3.11/site-packages/langchain/prompts/base.py:143\u001b[0m, in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    141\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m _get_jinja2_variables_from_template(template)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m template_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf-string\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 143\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    144\u001b[0m         v \u001b[38;5;28;01mfor\u001b[39;00m _, v, _, _ \u001b[38;5;129;01min\u001b[39;00m Formatter()\u001b[38;5;241m.\u001b[39mparse(template) \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     }\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported template format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_format\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Single '}' encountered in format string"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.5,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=chat,\n",
    "    max_token_limit=1000,\n",
    "    return_messages=True,\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"movie\": \"스타워즈\",\n",
    "        \"answer\": \"\"\"\n",
    "        🌌⚔️👽\n",
    "\n",
    "        - 🌌: 스타워즈는 우주 공간을 배경으로 한 환상적인 세계를 보여주는 영화이기 때문에 은은한 우주의 느낌을 담은 이모티콘을 선택했습니다.\n",
    "\n",
    "        - ⚔️: 영화에서 빛나는 레이저 검이나 전투 장면이 많이 등장하는데, 이를 상징하는 이모티콘으로 스타워즈의 전투적인 요소를 표현했습니다.\n",
    "\n",
    "    \n",
    "        - 👽: 스타워즈는 다양한 외계 생명체들이 등장하는데, 그 중에서도 다양한 외계인들이 등장하여 영화의 다양성을 나타내기 때문에 외계 생명체를 상징하는 이모티콘을 선택했습니다.\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"movie\": \"탑건\",\n",
    "        \"answer\": \"\"\"\n",
    "        🚀🔥😎\n",
    "        \n",
    "        🚀: '탑건'은 전투기 조종사들의 역동적인 비행씬과 스릴 넘치는 전투 장면으로 인해 마치 하늘을 나는 듯한 느낌을 주는 영화입니다. 전투기가 속도를 내며 하늘을 가로지르는 장면은 마치 로켓처럼 빠르고 감동적입니다.\n",
    "\n",
    "        🔥: 영화 속 주인공들의 열정과 용기가 불길처럼 뜨거운 에너지를 전달해줍니다. 전투기 조종사들이 마주하는 위험과 도전 속에서도 그들의 열정과 용기가 불을 지펴내는 듯한 느낌을 줍니다.\n",
    "\n",
    "        😎: '탑건'은 스타일리시한 비행복과 섹시한 룩으로 유명한 영화로, 주인공들의 카리스마 넘치는 매력이 관객을 사로잡습니다. 영화 속 주인공들은 마치 쿨한 선장처럼 매 순간을 멋지게 소화해내는 것이 인상적입니다.\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"movie\": \"반지의 제왕\",\n",
    "        \"answer\": \"\"\"\n",
    "        👑🧝‍♂️🔥\n",
    "\n",
    "        👑: '반지의 제왕'은 왕좌를 차지할 수 있는 힘을 상징하는 반지를 중심으로 전쟁과 운명의 이야기를 그린 영화로, 왕권과 권력에 대한 욕망을 상징하는 왕관을 상징합니다.\n",
    "\n",
    "        🧝‍♂️: 영화 속에 등장하는 엘프들은 우아하고 아름다운 존재로, 자연과 조화를 이루며 숲을 수호하는 역할을 합니다. 엘프는 영화의 신비로움과 순수함을 상징합니다.\n",
    "\n",
    "        🔥: 영화는 전투와 결투로 가득 차 있으며, 화려한 전투씬은 영화의 긴장감과 스릴을 높여줍니다. 불꽃같이 타오르는 전투는 캐릭터들의 용기와 희생을 상징합니다.        \n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"human\",\n",
    "            \"영화 {movie} 에 대해서 이모티콘 3개로 표현해줘. 그리고 그 이유에 대해서 알려줘.\",\n",
    "        ),\n",
    "        (\n",
    "            \"ai\",\n",
    "            \"{answer}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"당신은 영화의 내용을 {style} 표현하는 것으로 유명한 {role} 입니다. \",\n",
    "        ),\n",
    "        example_prompt,\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"영화 {movie} 에 대해서 이모티콘 3개로 표현해줘. 그리고 그 이유에 대해서 알려줘.\",\n",
    "        ),\n",
    "        # (\"ai\", \"{answer}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "general_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        example_prompt,\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "\n",
    "movie_chain = RunnablePassthrough.assign(chat_history=load_memory) | final_prompt | chat\n",
    "general_chain = (\n",
    "    RunnablePassthrough.assign(chat_history=load_memory) | general_prompt | chat\n",
    ")\n",
    "\n",
    "\n",
    "def invoke_movie_chain(movie):\n",
    "    result = movie_chain.invoke(\n",
    "        {\n",
    "            \"role\": \"웹툰 작가\",\n",
    "            \"style\": \"심미적으로\",\n",
    "            \"movie\": movie,\n",
    "        }\n",
    "    )\n",
    "    memory.save_context(\n",
    "        {\"input\": movie},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "\n",
    "\n",
    "def invoke_general_chain(question):\n",
    "    result = general_chain.invoke(\n",
    "        {\n",
    "            \"question\": question,\n",
    "        }\n",
    "    )\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "\n",
    "\n",
    "invoke_movie_chain(\"인생은 아름다워\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦖🔥🏙️\n",
      "\n",
      "🦖: '고질라'는 거대한 괴물인 고질라를 상징하는 이모티콘입니다. 영화 속 고질라는 인류에게 위협을 가하는 거대하고 강력한 존재로, 이를 나타내는 이모티콘입니다.\n",
      "\n",
      "🔥: 영화 속 고질라는 불길을 토해내며 도시를 파괴하는 장면이 인상적입니다. 고질라가 토해내는 불길은 파괴와 재앙을 상징하며, 영화의 긴장감과 스릴을 높여줍니다.\n",
      "\n",
      "🏙️: 도시를 배경으로 한 '고질라'는 거대한 괴물과 인간들의 사투를 그린 영화로, 도시의 높은 건물들과 현대 문명을 상징하는 이모티콘입니다. 고질라와 도시의 대결은 인류의 힘과 무력함을 보여줍니다."
     ]
    }
   ],
   "source": [
    "invoke_movie_chain(\"고질라\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='인생은 아름다워'),\n",
       " AIMessage(content=\"\\n        🎨👨\\u200d👦\\u200d👦😊\\n\\n        🎨: '인생은 아름다워'는 삶의 아름다움과 가치를 담은 영화로, 삶의 다양한 색채와 아름다움을 상징하는 이모티콘입니다. 영화 속 아름다운 풍경과 감동적인 순간들이 관객들에게 희망과 용기를 전달합니다.\\n\\n        👨\\u200d👦\\u200d👦: 영화는 가족과 사랑에 대한 이야기를 중심으로 전개됩니다. 주인공과 가족들 간의 따뜻한 인연과 사랑이 영화의 중요한 요소이며, 이를 나타내는 이모티콘입니다.\\n\\n        😊: '인생은 아름다워'는 삶의 가치와 소중함을 일깨워주는 영화로, 희망과 긍정적인 마음가짐을 전하는 내용을 담고 있습니다. 이로 인해 관객들은 영화를 보고 나서 행복한 미소를 지을 수 있습니다.        \\n        \"),\n",
       " HumanMessage(content='고질라'),\n",
       " AIMessage(content=\"🦖🔥🏙️\\n\\n🦖: '고질라'는 거대한 괴물인 고질라를 상징하는 이모티콘입니다. 영화 속 고질라는 인류에게 위협을 가하는 거대하고 강력한 존재로, 이를 나타내는 이모티콘입니다.\\n\\n🔥: 영화 속 고질라는 불길을 토해내며 도시를 파괴하는 장면이 인상적입니다. 고질라가 토해내는 불길은 파괴와 재앙을 상징하며, 영화의 긴장감과 스릴을 높여줍니다.\\n\\n🏙️: 도시를 배경으로 한 '고질라'는 거대한 괴물과 인간들의 사투를 그린 영화로, 도시의 높은 건물들과 현대 문명을 상징하는 이모티콘입니다. 고질라와 도시의 대결은 인류의 힘과 무력함을 보여줍니다.\")]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})[\"chat_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "죄송합니다. 이전 질문에 대한 정보를 기억하지 못합니다. 다시 한 번 질문을 해주시겠습니까?"
     ]
    }
   ],
   "source": [
    "invoke_general_chain(\"이전에 물어본 영화 제목이 무엇인가요?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
